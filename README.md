# Word Embeddings Notes

This folder contains various notes and notebooks exploring word embeddings techniques.

## Folder
As well as these notes, this folder contains:
* **word2vec-from-scratch.ipynb**: building my own neural network to create a _word2vec_ skip-gram model in order to understand how it works under the hood.
* **doc2vec-basics.ipynb**: toy example of using the _doc2vec_ implementation within gensim as an aide memoir.

## Notes
In short word embeddings map individual words to a vector. The aim is to construct an embedding such that similiar words are mapped to similiar vectors. 
Normally *similarity* is defined in terms of *semantic similarity*, words which are similiar in meaning rather than necessarily looking similiar. Most 
methods deduce semantic similarity by looking at whether words often appear with the same words around them.

Sebastian Ruder's [blog](http://ruder.io/word-embeddings-1/) is a great introduction to word embeddings, and Chris McCormick's [blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) contains a clear explaination of the archetypal word embedding approach _word2vec_. One interesting point that is discussed by Sebastian Ruder is the connection to pointwise mutual information.

One of the complications in discussing word embeddings is that colloquially the same term is often used to refer to the algorthimic approach to generating the word embedding, the most common implementation of the algorthim, or the most common pre-trained word embedding produced from the by the algorthim depending on context, even if seperate terms are available. Here I have tried to follow general usage rather than necessarily what is technically correct.

### Common approaches to word embeddings

The following are common word embeddings:
* **word2vec**: this uses a shallow neural network to map words to vectors. One important point about this neural network is that the a linear activation function is used, this means that the vector addition works with the resulting vectors. A version of this algorithm is available in gensim and pre-trained vectors are also [available](https://code.google.com/archive/p/word2vec/) where the vectors have been trained on Google News Group data. There is [evidence](https://arxiv.org/abs/1607.06520) that the pre-trained vectors have learnt gender biases from this News Group data. These biases are likely to be an issue with many word embeddings.
* **GloVe**: this is an alternative to _word2vec_ that is not based on neural networks but instead is constructed from statistics on the co-occurance of words. Information about _GloVe_ can be found [here](https://nlp.stanford.edu/projects/glove/) as can a range of pre-trained vectors. It is not clear whether _word2vec_ or _GloVe_ is more performant on given problems. 
* **fastText**: _FastText_ was created by the same research group as _word2vec_ and has been opened source by facebook. More information and pre-trained vectors can be found [here](https://github.com/facebookresearch/fastText). _FastText_ creates word embeddings that take into account the subsets of characters a word. For example, it might learn that having the letters "im" at the start of a word tells you something about the meaning of that word. This also means that _fastText_ can create vectors for words not appearing in the original training data by considering the different subsets of characters within that word.
* **Conceptnet Numberbatch**: this combines information from various embeddings (including commonly used pre-trained _word2vec_ and _GloVe_ vectors) and also ccombines it with knowledge of associations between words captured by the _ConceptNet_ project. More information can be found [here](https://github.com/commonsense/conceptnet-numberbatch) and the authors also argue that their pre-trained embeddings contain fewer biases than those within other commonly used embeddings. 
* **sense2vec**: The aim of _sense2vec_ is to deal with the case where a word has more than one distinct meaning. The idea is to append annotations to each word which denote which meaning is indicated in any given situation. In order to create annotations algorithmically NLP techniques (in particular part-of-speech tagging and named entity detection) are used to detect how a word is being used. Once annotations have been appended to each and every word, these new words can be run through standard word embeddings algorithms such as _word2vec_ and _GloVe_. See this [blog post](https://explosion.ai/blog/sense2vec-with-spacy) and code their [code repository](https://github.com/explosion/sense2vec) for more details.


The vectors from word embeddings can be used to look for similiar words and this is typically found by calculating the cosine distance between word vectors. Cosine distance only considers the angles between vectors, but the magnitude of the vectors can also have meaning. As shown in [this paper](https://arxiv.org/abs/1508.02297), with _word2vec_ the length of the vector is related to the both the number of times a word occurs and whether the word is always appearing in very similiar contexts.

TensorBoard (online version [here](http://projector.tensorflow.org/)) has a visualisation of the Google News Group _word2vec_ embeddings built and is a good way of understanding what sort of words are flagged as being similiar. You can also read in your own embeddings if they are in the right format.

### Combining embeddings

Papers such as [this](https://pdfs.semanticscholar.org/343d/39534682bb7b2eec14f573360877eb80cd59.pdf) and [this](https://arxiv.org/abs/1604.01692) discuss ways in which different word embeddings can be combined. Methods include:
* Simply concatenating the vectors together for each word.
* Concatenate the vectors and then perform singular value decomposition.
* Concatenate the vectors and then use an autoencoder to reduce the number of dimensions.

As discussed in the Conceptnet Numberbatch [paper](https://arxiv.org/abs/1604.01692) a difficultly is dealing with words that appear in the vocabulary of one embedding but not the other. The way they deal with this is if a word is missing they look at how the vectors of the most similiar words transform between the two embeddings, and the angle between the vector of the word in question and these similiar words in the original space.

Another thing that the Conceptnet Numberbatch embedding does is take into account known associations between words and encodes them into a matrix, this the word embeddings matrix is then multiplied by this associations matrix. To demonstrate the principle (the full approach is more complicated) consider an embedding of dimension three containing just three words where we know _a priori_ that words one and three are basically the same. Multiplying the embeddings matrix by the association matrix shown results in word one and word three having the same vector.
   ![equation](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D%201%20%26%200%20%26%20%5Cfrac12%20%5C%5C%200%20%26%201%20%26%200%20%5C%5C%20%5Cfrac12%20%26%200%20%26%201%20%5Cend%7Bbmatrix%7D%20%5Cbegin%7Bbmatrix%7D%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word1%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5C%5C%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word2%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5C%5C%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word3%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5Cend%7Bbmatrix%7D%20%3D%20%5Cbegin%7Bbmatrix%7D%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word1%7D&plus;%5Ctextrm%7B%5Cbf%20word3%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5C%5C%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word2%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5C%5C%20%5Ctextrm%7B%20---%7D%20%26%20%5Ctextrm%7B%5Cbf%20word1%7D&plus;%5Ctextrm%7B%5Cbf%20word3%7D%20%26%20%5Ctextrm%7B---%20%7D%20%5Cend%7Bbmatrix%7D)

The associations do not have to be as strong as that shown in this example.

Related to the idea of combining embeddings is that vocabulary expansion, where one embedding could be used to fill in missing words from another embedding. As well as the approaches above, [this paper](https://arxiv.org/pdf/1506.06726.pdf) discusses how a unitary transformation can be learnt that maps one to another (though in that paper it is between word embeddings constucted in the same way but for different languages in order to do translation).

### Creating document level embeddings

Often we are interested less in similarity betwen individual vectors but between whole chunks of text, be it a sentence, a paragraph, a whole document or ssome other of collections of sequential words. Here these will be referred to generically as _documents_. Such approaches include:

* **Aggregating word vectors**: one common approach approach is to sum all the words within a document to produce a document vector (more probably to take the mean so that varying lengths of documents do not affect the results).. This can be done more sophisticatedly by weighting the significance of each word (for example with TF-IDF weights) but, as discussed above, the length of a word vector can contain information about significance so you need to think carefully about whether to normalise the word vectors or not (for example, if you are going to apply your own TF-IDF weights you probably want to normalise the word vectors). The motivation for this approach is that one of the majoy successes of word embeddings is that vector addition of word vectors has meaning (the classic example being **king**-**man**+**women**~**queen**).
* **Paragraph vectors**: Paragraph vectors are often referred to as **doc2vec**. The idea is to use _word2vec_ style neural networks but train a document vector alongside of training for word vectors. To oversimply, _word2vec_ creates word vectors from trying to predict words from the words that appear around them whereas _doc2vec_ creates word and document vectors from trying to predict words from both the words appearing around them and a document vector that is a constant vector for a given document. You can also use the same approach to produce vectors for a set of documents (for instance a topic) by telling it that a series of documents have the same label. One implementation of _doc2vec_ for Python can be found within gensim. The original paper can be found [here](https://cs.stanford.edu/~quocle/paragraph_vector.pdf).
* **lda2vec**: this combines techniques from Latent Dirichlet Allocation (LDA) topic modelling and from paragraph vectors. The orginal paper can be found [here](https://arxiv.org/abs/1605.02019) and a good blog post explaining the ideas can be found [here](http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term=). The idea is similiar to paragraph vectors but the document vector you train is written as a linear combination of _n_ topic vectors, where _n_ is supplied by the users. 
* **Skip-thought vectors**: this extends the central idea dehind _word2vec_ (of predicting words from their nearby words) to whole sentences. One produce word vectors that map to document vectors but these have been trained by looking to predict a sentence from sentences that surround it. This is described as an encoder-decoder model as the encoder maps the words to a sentence vector and the decoder takes a sentence vector and predicts the sentence before and after the sentence in question. The corpus used to train this data consists of books, but mostly romance and fantasy books. One implementation of this approach in Python is called _sent2vec_ and can be found [here](https://github.com/ryankiros/skip-thoughts). But confusingly make other people have used the term _sent2vec_ for similiar things, Microsoft has something with this name ([see here](https://www.microsoft.com/en-us/research/project/dssm/)) as does [this paper](https://arxiv.org/pdf/1703.02507.pdf) which seems to create word vectors and ngram vectors using _word2vec_ style techniques and then creates sentence vectors through aggregation. 

A few of these approaches are discussed in this [blog post](https://www.kernix.com/blog/similarity-measure-of-textual-documents_p12).

An alternative approach that still uses word embeddings is to consider **word mover distance**. This allows different sentences to be compared for similarity by an approach that has similarities to edit distance. One of the examples in [the paper](http://proceedings.mlr.press/v37/kusnerb15.pdf) is to consider the two sentences *Obama speaks to the media in Illinois* and *The President greets the the press in Chicago* the distance between these two sentences is the sum distance between equivalent words (ignoring stop words), where this distance between the words _word2vec_ vectors. In this case that would be the sum of the distances between *Obama* and *President*; *speaks* and *greets*; *media* and *press*; and *Illinois* and *Chicago*. This is very computationaly expensive to compute so is best used to compare a small number of short sentences rather than whole documents. 

These word embedding techniques are alternatives to performing techniques such as **Latent Semantic Analysis (LSA)**. LSA is a bag of words approach where the number of times each word in the vocabulary occurs is tabulated. These term frequencies can then be weighted if warranted (such as with TF-IDF) and then singular value decomposition can be performed to reduce the number of dimensions. This would produce a vector for each document, but words with the same meaning would have been treated as separate features. 
