{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec from scratch\n",
    "\n",
    "Word2vec word embeddings make use of neural networks to learn how to map words into vectors in such a way that words that appear in similiar contexts have similiar vectors.\n",
    "\n",
    "Various libraries (such as gensim) allow you to build your own word2vec models and various pre-trained word embeddings are available to download online. Also, neural network libraries (such as keras) often allow you to train an embedding as the first layer of a neural network. These pre-trained vectors have often been trained on a very large corpus with a carefully trained neural network so in practice are the best place to start unless there is a compelling reason not to. Neural network libraries (such as keras) often allow you to train an embedding as the first layer of a neural network.\n",
    "\n",
    "However, in order to aid my own understanding of how word2vec works this notebook sets out build my own skip-gram word word embeddings by hand constructing a neural network. This is based on the [Chris McCormick's blog post.](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "This notebook is purely for explorely the inner workings of word2vec, it is **not** something to be used for generating a useable word2vec model (which is pretty simple in gensim). You will notice that I had to restrict the amount of text considered to avoid memory issues. This could have mitigated by the use of sparse vectors and by limited the number of words to the most commonly occuring ones, but for clarity these steps have been ommitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For getting the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# For cleaning the text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# For training the neural network\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# For displaying the neural network\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "Here some of the newsgroups data is downloaded using the sklearn function. Headers and footers have been removed, and only 150 newsgroup posts are actually used to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another fish to check out is Richard Rast -- h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: As the subject says - Can I use a 4052 for d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am looking for current sources for lists of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nBut why do you characterize this as a \"fli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIt was more than a theoretical concept; it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\n\\nThe name is rather descriptive.  It's a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My mom has just been diagnosed with cystic bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nThe yearly chest x-ray provides a minute a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I've recently listened to a tape by Dr. Stanis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We've just been donated a large machine for us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   Another fish to check out is Richard Rast -- h...\n",
       "1   : As the subject says - Can I use a 4052 for d...\n",
       "2   I am looking for current sources for lists of ...\n",
       "3   \\n\\nBut why do you characterize this as a \"fli...\n",
       "4   \\nIt was more than a theoretical concept; it w...\n",
       "5   \\n\\n\\nThe name is rather descriptive.  It's a ...\n",
       "6   My mom has just been diagnosed with cystic bre...\n",
       "7   \\n\\nThe yearly chest x-ray provides a minute a...\n",
       "9   I've recently listened to a tape by Dr. Stanis...\n",
       "10  We've just been donated a large machine for us..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catagories = ['sci.med', 'sci.electronics', 'sci.space']\n",
    "text_data = fetch_20newsgroups(categories=catagories,\n",
    "                               random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['text'] = text_data.data\n",
    "\n",
    "# Remove blank lines\n",
    "df = df[ df['text']!= \"\" ]\n",
    "\n",
    "# Restrict to 150 rows\n",
    "df = df.head(150)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the text\n",
    "\n",
    "Next we define a tokeniser that aswell as splitting a string of text into individual tokens, also removed punctuation, stopwords and numbers. In addition words have been stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    \n",
    "    # Remove any whitespace at the start and end of the string\n",
    "    # and remove any stray tabs and newline characters\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove any weird unicode characters\n",
    "    if isinstance(text, unicode):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "        \n",
    "    # Convert hyphens and slashes to spaces\n",
    "    text = re.sub(r'[-/]+',' ',text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\S*\\d+\\S*', '', text)\n",
    "    \n",
    "    # Remove remaining punctuation\n",
    "    text = text.translate(None, string.punctuation)\n",
    "    \n",
    "    # Convert the text to lowercase and use nltk tokeniser\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Define a list of stopwords apart from the word 'not'\n",
    "    stops = set(stopwords.words('english')) - set(('not'))\n",
    "\n",
    "    # Define stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    return [str(stemmer.stem(i)) for i in tokens if i not in stops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apply this function to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Another fish to check out is Richard Rast -- h...</td>\n",
       "      <td>[anoth, fish, check, richard, rast, work, lock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: As the subject says - Can I use a 4052 for d...</td>\n",
       "      <td>[subject, say, use, digit, signal, dont, see, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am looking for current sources for lists of ...</td>\n",
       "      <td>[look, current, sourc, list, home, medic, test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nBut why do you characterize this as a \"fli...</td>\n",
       "      <td>[character, flight, fanci, fantasi, unfamiliar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIt was more than a theoretical concept; it w...</td>\n",
       "      <td>[theoret, concept, serious, pursu, freeman, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\n\\nThe name is rather descriptive.  It's a ...</td>\n",
       "      <td>[name, rather, descript, command, spacecraft, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My mom has just been diagnosed with cystic bre...</td>\n",
       "      <td>[mom, diagnos, cystic, breast, diseas, big, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nThe yearly chest x-ray provides a minute a...</td>\n",
       "      <td>[year, chest, x, ray, provid, minut, amount, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I've recently listened to a tape by Dr. Stanis...</td>\n",
       "      <td>[ive, recent, listen, tape, dr, stanislaw, bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We've just been donated a large machine for us...</td>\n",
       "      <td>[weve, donat, larg, machin, use, robot, lab, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Another fish to check out is Richard Rast -- h...   \n",
       "1   : As the subject says - Can I use a 4052 for d...   \n",
       "2   I am looking for current sources for lists of ...   \n",
       "3   \\n\\nBut why do you characterize this as a \"fli...   \n",
       "4   \\nIt was more than a theoretical concept; it w...   \n",
       "5   \\n\\n\\nThe name is rather descriptive.  It's a ...   \n",
       "6   My mom has just been diagnosed with cystic bre...   \n",
       "7   \\n\\nThe yearly chest x-ray provides a minute a...   \n",
       "9   I've recently listened to a tape by Dr. Stanis...   \n",
       "10  We've just been donated a large machine for us...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [anoth, fish, check, richard, rast, work, lock...  \n",
       "1   [subject, say, use, digit, signal, dont, see, ...  \n",
       "2   [look, current, sourc, list, home, medic, test...  \n",
       "3   [character, flight, fanci, fantasi, unfamiliar...  \n",
       "4   [theoret, concept, serious, pursu, freeman, dy...  \n",
       "5   [name, rather, descript, command, spacecraft, ...  \n",
       "6   [mom, diagnos, cystic, breast, diseas, big, re...  \n",
       "7   [year, chest, x, ray, provid, minut, amount, r...  \n",
       "9   [ive, recent, listen, tape, dr, stanislaw, bur...  \n",
       "10  [weve, donat, larg, machin, use, robot, lab, d...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['text'].apply(lambda x: tokeniser(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to numbers\n",
    "\n",
    "In order for the tokens to be used within a model we need to convert the text into numbers. The first step is to create a list of all the unique words appearing in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the tokens\n",
    "all_words = df['tokens'].tolist()\n",
    "\n",
    "# Flatten\n",
    "all_words = [ value for row in all_words for value in row]\n",
    "\n",
    "# Remove duplicates\n",
    "all_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is create dictionaries that link the words to a number (and vice versa). The number of unique words is also recorded as this will prove useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {k: v for v, k in enumerate(all_words)}\n",
    "num_to_word = {v: k for v, k in enumerate(all_words)}\n",
    "\n",
    "num_words = len(word_to_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras does not want to be given a number between *0* and *num_words* to describe which word it is seeing. Instead it expects an array of length *num_words*, where if we are considering the word whose associated number is given by *i* the *i*th element of the array has value one.\n",
    "\n",
    "Below we define a function which takes a word appearing in the training corpus as input (where it has been cleaned and stemmed) and outputs the associated array in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    features = np.zeros( len(word_to_num) )\n",
    "    features[ word_to_num[word] ] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the problem into one of prediction\n",
    "\n",
    "As discussed in [Chris McCormick's blog post.](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) the key part of  word2vec is the transformation of the problem of creating a new representation for words into one of prediction. The blog neatly explains the concepts (with diagrams) so only a brief outline is presented here.\n",
    "\n",
    "The first step is to apply a rolling window to each row of data. If the window size is set to five, for example, the two words precedding a given word and the two words following a given word are considered.\n",
    "\n",
    "The prediction challenge then becomes: given the preceeding (or following) word predict the middle word.\n",
    "\n",
    "So if our window was of size three and looked liked:\n",
    "    $$x_0, \\, x_1, \\, x_2, \\, x_3, \\, x_4$$\n",
    "\n",
    "We would be have four data points:\n",
    "    $$\\left( x_0, x_2\\right), \\, \\left( x_1, x_2\\right), \\, \\left( x_3, x_2\\right), \\, \\left( x_4, x_2\\right)$$\n",
    "\n",
    "where we are trying to predict the second number from each pair from the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the features\n",
    "\n",
    "The first step is to apply this rolling window and create these input / ouput pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from the first post are:\n",
      "['anoth', 'fish', 'check', 'richard', 'rast', 'work', 'lockhe', 'missil', 'site', 'nasa', 'johnson', 'nick', 'johnson', 'kaman', 'scienc', 'colo', 'spgs', 'friend', 'darren', 'mcknight', 'kaman', 'alexandria', 'va', 'good', 'luck', 'r', 'landi']\n",
      "\n",
      "The first few input / output pairs are:\n",
      "[('fish', 'anoth'), ('check', 'anoth'), ('anoth', 'fish'), ('check', 'fish'), ('richard', 'fish')]\n"
     ]
    }
   ],
   "source": [
    "# Define the window size\n",
    "window = 5\n",
    "\n",
    "# Halve the window size\n",
    "half_wind = int(window/2)\n",
    "\n",
    "\n",
    "input_output = []\n",
    "\n",
    "# Loop over each post\n",
    "for row in df['tokens']:\n",
    "    \n",
    "    # Loop over each word in the post and consider the window around it\n",
    "    for i, word in enumerate(row):\n",
    "    \n",
    "        # Look at the preceeding half window\n",
    "        for j in xrange(i-half_wind, i):\n",
    "            if(j >= 0):\n",
    "                input_output.append( (row[j], row[i]) )\n",
    "    \n",
    "        # Look at the following half window\n",
    "        for j in xrange(i+1, i+half_wind+1):\n",
    "            if(j< len(row)):\n",
    "                input_output.append( (row[j], row[i]) )\n",
    "\n",
    "print \"Tokens from the first post are:\"\n",
    "print df['tokens'][0]\n",
    "print \"\"\n",
    "print \"The first few input / output pairs are:\"\n",
    "print input_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next these inputs and outputs need to be encoded into the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for tup in input_output:\n",
    "    X.append(encode_word(tup[0]))\n",
    "    y.append(encode_word(tup[1]))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the expected format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X value is:\n",
      "fish\n",
      "\n",
      "First feauture is:\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "Which index of the first feature has a one?\n",
      "2276\n",
      "\n",
      "What word does this argument correspond to?\n",
      "fish\n"
     ]
    }
   ],
   "source": [
    "print \"First X value is:\"\n",
    "print input_output[0][0]\n",
    "\n",
    "print \"\"\n",
    "\n",
    "print \"First feauture is:\"\n",
    "first_feat = X[0,:]\n",
    "print first_feat\n",
    "\n",
    "print \"\"\n",
    "\n",
    "print \"Which index of the first feature has a one?\"\n",
    "first_arg = np.argwhere(first_feat)[0][0]\n",
    "print first_arg\n",
    "\n",
    "print \"\"\n",
    "print \"What word does this argument correspond to?\"\n",
    "print num_to_word[first_arg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network\n",
    "\n",
    "Next build the neural network. The first step is to specify the layers, the number of neurons in the first layer determines the number of dimensions of the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"221pt\" viewBox=\"0.00 0.00 277.00 221.00\" width=\"277pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-217 273,-217 273,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139677754295248 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139677754295248</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 269,-212.5 269,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-185.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"125,-166.5 125,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"125,-189.5 180,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"180,-166.5 180,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-197.3\">(None, 3959)</text>\n",
       "<polyline fill=\"none\" points=\"180,-189.5 269,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-174.3\">(None, 3959)</text>\n",
       "</g>\n",
       "<!-- 139676179749200 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139676179749200</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-83.5 11.5,-129.5 257.5,-129.5 257.5,-83.5 11.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-102.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-83.5 113.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-106.5 168.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-83.5 168.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213\" y=\"-114.3\">(None, 3959)</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-106.5 257.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139677754295248&#45;&gt;139676179749200 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139677754295248-&gt;139676179749200</title>\n",
       "<path d=\"M134.5,-166.366C134.5,-158.152 134.5,-148.658 134.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"138,-139.607 134.5,-129.607 131,-139.607 138,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139676163727440 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139676163727440</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-0.5 11.5,-46.5 257.5,-46.5 257.5,-0.5 11.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-0.5 113.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"113.5,-23.5 168.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-0.5 168.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"168.5,-23.5 257.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"213\" y=\"-8.3\">(None, 3959)</text>\n",
       "</g>\n",
       "<!-- 139676179749200&#45;&gt;139676163727440 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139676179749200-&gt;139676163727440</title>\n",
       "<path d=\"M134.5,-83.3664C134.5,-75.1516 134.5,-65.6579 134.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"138,-56.6068 134.5,-46.6068 131,-56.6069 138,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(num_words,))\n",
    "x = Dense(100, activation=None)(inputs)\n",
    "predictions = Dense(num_words, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fit the model. The number of epochs has been constrained to keep execution time down (it is only a toy model after all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "61472/61472 [==============================] - 23s - loss: 7.7475 - acc: 0.0083    \n",
      "Epoch 2/5\n",
      "61472/61472 [==============================] - 23s - loss: 7.5875 - acc: 0.0087    \n",
      "Epoch 3/5\n",
      "61472/61472 [==============================] - 23s - loss: 7.5409 - acc: 0.0096    \n",
      "Epoch 4/5\n",
      "61472/61472 [==============================] - 23s - loss: 7.4795 - acc: 0.0123    \n",
      "Epoch 5/5\n",
      "61472/61472 [==============================] - 23s - loss: 7.4105 - acc: 0.0150    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f090f748e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the weights\n",
    "\n",
    "The word2vec model is just the weights from the first layer of the trained neural network. This can be extracted from the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3959)\n"
     ]
    }
   ],
   "source": [
    "word2vec = model.layers[2].get_weights()[0]\n",
    "print word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *i*th column of this matrix is the vector associated with the *i*th word. A function can be written to take a word that appears in the corpus (cleaned and stemmed) that returns the associated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.97141325,  0.67672294, -0.96810716, -0.84520632, -0.41413036,\n",
       "        1.11274695, -2.10097265,  1.38101363, -0.64868277,  1.04028881,\n",
       "       -0.94923067, -0.59151566,  1.25863171, -2.06790137, -0.8669197 ,\n",
       "       -0.74480015,  1.18164492,  0.79420584,  0.75332636,  1.1417706 ,\n",
       "       -2.08621573, -0.63764066,  0.6852352 ,  1.73760247, -1.02486575,\n",
       "       -0.80407125,  1.61554611, -0.80632335, -1.73476946,  2.08853555,\n",
       "        1.83512485, -1.36123836, -1.65640235,  0.48223022, -0.70910507,\n",
       "        1.85455203, -0.75441939,  1.77476239,  1.04991508, -1.22752595,\n",
       "       -0.84015119, -0.88672745, -2.08637238,  0.28832033, -1.36546981,\n",
       "        1.09845233, -0.62574017, -1.39769268,  1.57041049,  1.36418021,\n",
       "        2.20699096,  1.98577929,  2.18315649,  0.27575612,  0.66500497,\n",
       "        1.14938056, -1.111305  ,  1.04218936,  1.28551495,  1.08309782,\n",
       "       -1.06331027, -0.50949413, -2.11069012, -0.86842036, -2.04522061,\n",
       "        1.24561775, -1.55559301, -0.30805171,  2.18879652, -1.35909212,\n",
       "        0.80913293, -2.26613927, -0.50109965, -1.17366362, -1.74624932,\n",
       "       -1.44851184, -0.97931725, -0.68889052,  1.22289491,  0.75443935,\n",
       "       -1.34617674, -2.20317554,  1.57622147, -0.92225659, -0.83068788,\n",
       "       -0.71767551,  1.32306445, -0.79265392, -0.9784168 ,  0.87064594,\n",
       "        1.65198362,  0.55874336,  1.441172  ,  1.3227514 ,  0.68868595,\n",
       "        0.4638631 ,  0.98623133,  2.19779778, -1.18828964,  0.8904742 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lookup_word(word):\n",
    "    return word2vec[:, word_to_num[word]]\n",
    "\n",
    "lookup_word('fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates some of the things going on under-the-hood when a skip-gram word2vec model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
